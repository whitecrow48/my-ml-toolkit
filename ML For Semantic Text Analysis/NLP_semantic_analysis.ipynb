{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5479c1-f4a5-4619-a93e-4751ee84b356",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Применение мамшинного обучения для смыслового анализа текста\n",
    "## Подготовка набора данных с обзорами фильмов на IMDB\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Обработка естественного языка (Natural Language Processing, NLP)\n",
    "**NLP** - так называемый анализ эмоциональной окраски текстов (анализ мнений или анализ настроений), с помощью которого мы можем максимально эффективно извлекать полезные данные для анализа.\n",
    "\n",
    "**Цель** - взять 50.000 отзывов на IMDB, извлечь значимую информацию из подмножества обзоров фильмов и построить модель ML, способную предсказать, понравится или не понравится фильм определенному пользователю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1c4242d9-1bb8-476d-a227-f4f913ceed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868fdc61-e08a-4172-8031-9e3a8a3f7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Загрузка отзывов: 100%|████████████████| 50000/50000 [00:00<00:00, 59278.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Соберем отдельные текстовые документы в один файл CSV\n",
    "\n",
    "basepath = os.path.expanduser('~/ML/Data/aclImdb')\n",
    "labels = {\"pos\": 1, \"neg\": 0}\n",
    "\n",
    "data = []\n",
    "\n",
    "# Считаем общее количество файлов\n",
    "total_files = sum(len(os.listdir(os.path.join(basepath, s, l))) \n",
    "                  for s in ('train', 'test') \n",
    "                  for l in ('pos', 'neg'))\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Загрузка отзывов\") as pbar:\n",
    "    for s in ('train', 'test'):\n",
    "        for l in ('pos', 'neg'):\n",
    "            folder = os.path.join(basepath, s, l)\n",
    "            for filename in os.listdir(folder):\n",
    "                with open(os.path.join(folder, filename), 'r', encoding='utf-8') as f:\n",
    "                    review = f.read()\n",
    "                data.append([review, labels[l]])\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['review', 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a9223-b586-4b99-b248-5d46334bf72c",
   "metadata": {},
   "source": [
    "Используя вложенные циклы, мы прошлись по подкаталогам `train` и `test`, в главном каталоге `aclImdb` и прочитали отдельные текстовые файлы из подкаталогов `pos` и `neg`, которые добавили к набору данных `df` вместе с целочисленной меткой класса (1 = положительный и 0 = отрицательный).\n",
    "\n",
    "Так как метки классов в собраном наборе данных упорядочены, надо их перемешать, что полезно для разделения набора данных на `train` и `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fbb459-312e-4eaf-92f0-5f169e6d21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  This movie is goofy as hell! I think it was wr...          1\n",
      "1  Kurosawa, fresh into color, losses sight of hi...          1\n",
      "2  A very addictive series.I had not seen an exac...          1\n",
      "3  An excellent cast makes this movie work; all o...          1\n",
      "4  This is one of the most brilliant movies that ...          1\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc11124-e5f4-4fd6-adbe-e5e787daa8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним собранный и перетасованный набор данных в CSV\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('~/ML/Data/movie_data.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e971d8d0-c23d-419c-9ff6-a4f3a87dedc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just want to add my two cents worth, and for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This video nasty was initially banned in Brita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The original GRUDGE (the original American rem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I just want to add my two cents worth, and for...          1\n",
       "1  This video nasty was initially banned in Brita...          0\n",
       "2  The original GRUDGE (the original American rem...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удостоверимся, что сохранили данные в нужном формате\n",
    "df = pd.read_csv('~/ML/Data/movie_data.csv', encoding = 'utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c127ed73-b07f-49e8-add8-514d5984be90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удостоверимся, что DataFrame содержит все строки\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bf877-c3e0-4b4c-86e2-a955a8eca25d",
   "metadata": {},
   "source": [
    "## Знакомство с моделью мешка слов\n",
    "\n",
    "### Модель \"мешок слов\" (Bag-of-Words, BoW)\n",
    "\n",
    "#### Преобразование категориальных данных\n",
    "\n",
    "* Перед передачей алгоритму машинного обучения **категориальные данные** (например, текст или слова) нужно **преобразовать в числовую форму**.\n",
    "\n",
    "\n",
    "#### Идея модели мешка слов\n",
    "\n",
    "* **Цель:** представить текст в виде **вектора числовых признаков**.\n",
    "* **Основная идея:** не учитывать порядок слов, а только их наличие и частоту.\n",
    "\n",
    "##### Этапы:\n",
    "\n",
    "1. **Создание словаря уникальных токенов**\n",
    "\n",
    "   * Из всего набора документов формируется словарь уникальных слов.\n",
    "\n",
    "2. **Построение вектора признаков для каждого документа**\n",
    "\n",
    "   * Вектор содержит **подсчет количества каждого слова** из словаря в данном документе.\n",
    "   * Так как каждый документ использует только часть словаря, **вектор преимущественно состоит из нулей** → такие векторы называются **разреженными**.\n",
    "\n",
    "\n",
    "#### Термины\n",
    "\n",
    "* **Raw Term Frequencies (t.f{t, d}):**\n",
    "\n",
    "  * Количество раз, когда термин $t$ встречается в документе $d$.\n",
    "* **Особенность модели:**\n",
    "\n",
    "  * Порядок слов в документе **не имеет значения**.\n",
    "  * Порядок значений в векторе определяется **индексами словаря** (обычно в алфавитном порядке).\n",
    "\n",
    "\n",
    "#### Инструмент для реализации\n",
    "\n",
    "* **CountVectorizer** (например, в Python/Scikit-learn) позволяет автоматически:\n",
    "\n",
    "  * Создать словарь;\n",
    "  * Преобразовать документы в разреженные векторы частот слов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338928d4-1cbd-426e-a7f7-b59dda1f4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'солнце': 7, 'светит': 6, 'погода': 4, 'прекрасная': 5, 'один': 2, 'плюс': 3, 'будет': 0, 'два': 1}\n"
     ]
    }
   ],
   "source": [
    "# Пример\n",
    "\n",
    "# Строим словарь уникальных токенов из всего набора даныых\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['Солнце светит',\n",
    "                 'Погода прекрасная',\n",
    "                 'Солнце светит, погода прекрасная, а один плюс один будет два'])\n",
    "\n",
    "# Построим словарь мешка слов и преобразуем в разряженные векторы признаков\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a823a-d0d0-4a73-a6e8-06694dcce779",
   "metadata": {},
   "source": [
    "CountVectorizer сканирует все документы (docs) и находит уникальные слова (токены), в нашем случае уникальные слова.\n",
    "\n",
    ">Индексы могут выглядеть произвольно (здесь CountVectorizer сам распределил их), но главное: каждое слово имеет уникальный индекс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08702a59-bb62-4cd4-b446-83396d9df7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 1 0 0]\n",
      " [1 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Векторы признаков\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c915579-021a-4c9c-9b50-7dc4b6ccc41d",
   "metadata": {},
   "source": [
    "### 2. Построение разреженных векторов признаков\n",
    "\n",
    "`bag.toarray()` показывает **вектор частот слов для каждого документа**.\n",
    "\n",
    "#### Документ 1: `'Солнце светит'`\n",
    "\n",
    "* Слова: `['солнце', 'светит']`\n",
    "* Вектор `[0 0 0 0 0 0 1 1]`\n",
    "\n",
    "  * Индекс 6 → `'светит'` встречается 1 раз\n",
    "  * Индекс 7 → `'солнце'` встречается 1 раз\n",
    "  * Остальные слова не встречаются → 0\n",
    "\n",
    "#### Документ 2: `'Погода прекрасная'`\n",
    "\n",
    "* Вектор `[0 0 0 0 1 1 0 0]`\n",
    "\n",
    "  * Индекс 4 → `'погода'` = 1\n",
    "  * Индекс 5 → `'прекрасная'` = 1\n",
    "  * Остальные слова = 0\n",
    "\n",
    "#### Документ 3: `'Солнце светит, погода прекрасная, а один плюс один будет два'`\n",
    "\n",
    "* Вектор `[1 1 2 1 1 1 1 1]`\n",
    "\n",
    "  * `'будет'` (индекс 0) = 1\n",
    "  * `'два'` (индекс 1) = 1\n",
    "  * `'один'` (индекс 2) = 2 (слово встречается дважды)\n",
    "  * `'плюс'` (индекс 3) = 1\n",
    "  * `'погода'` (индекс 4) = 1\n",
    "  * `'прекрасная'` (индекс 5) = 1\n",
    "  * `'светит'` (индекс 6) = 1\n",
    "  * `'солнце'` (индекс 7) = 1\n",
    "\n",
    "\n",
    "### Вывод\n",
    "\n",
    "* **Каждый документ** представлен **вектором длиной = числу уникальных слов во всём корпусе**.\n",
    "* **Значения вектора = количество раз, которое слово встречается в документе** (raw term frequency).\n",
    "* **Порядок слов в тексте не важен** — только частота.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55879547-9fcb-4fc2-8a44-22a09df88838",
   "metadata": {},
   "source": [
    "### N-граммы\n",
    "\n",
    "* **1-грамма (униграмма):** каждое слово отдельно.\n",
    "\n",
    "  * Пример: `\"the sun is shining\"` → `\"the\"`, `\"sun\"`, `\"is\"`, `\"shining\"`\n",
    "* **2-грамма:** пары слов.\n",
    "\n",
    "  * Пример: `\"the sun\"`, `\"sun is\"`, `\"is shining\"`\n",
    "* **n-грамма:** последовательность из n элементов.\n",
    "* В Python (`CountVectorizer`) можно задать n через `ngram_range`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f53299-0f8a-4b40-a56b-b3f6d480cc3d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Оценка релевантности слов с помощью частоты термина и обратной связи\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Бывает так, что мы можем столкнуться со словами, которые встречаются в документах обоих классов, и они могут не содержать полезной информации, именно для этого случая существует метод **Обратная частота документа (TF-IDF)**\n",
    "\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "* **Идея:** уменьшить вес часто встречающихся слов, которые мало помогают различать документы.\n",
    "* **Формула:**\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t, d)\n",
    "$$\n",
    "\n",
    "* **tf(t, d)** — частота слова $t$ в документе $d$\n",
    "* **idf(t, d)** — обратная частота документа:\n",
    "\n",
    "$$\n",
    "\\text{idf}(t, d) = \\log \\frac{n_d}{1 + df(d, t)}\n",
    "$$\n",
    "\n",
    "* $n_d$ — общее число документов в датасете\n",
    "\n",
    "* $df(d, t)$ — число документов, где встречается слово $t$\n",
    "\n",
    "* +1 в знаменателе и логарифм предотвращают слишком большой вес редких слов.\n",
    "\n",
    "* В Python: `TfidfTransformer` берёт частоты из `CountVectorizer` и преобразует их в TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb495a1a-1b45-4a28-8905-fc3c6a85e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.71 0.71]\n",
      " [0.   0.   0.   0.   0.71 0.71 0.   0.  ]\n",
      " [0.33 0.33 0.66 0.33 0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# Преобразование в TF-IDF необработанных частот терминов\n",
    "tfidf = TfidfTransformer(use_idf = True, # Назначить нулевой вес\n",
    "                         norm = 'l2', # L2-нормализация\n",
    "                         smooth_idf = True)\n",
    "np.set_printoptions(precision = 2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de790b-a2b6-44b5-8281-e63ac14cd84f",
   "metadata": {},
   "source": [
    "* **Нормализация TF-IDF** нужна, чтобы сравнивать документы разной длины.\n",
    "* **TfidfTransformer** делает это автоматически (по умолчанию **L2-норма**).\n",
    "* **L2-норма:** делит вектор признаков на его длину, чтобы итоговый вектор имел длину 1:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{\\text{norm}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2} = \\frac{\\mathbf{v}}{\\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}}\n",
    "$$\n",
    "\n",
    "* Это позволяет сравнивать документы по **направлению векторного пространства**, а не по абсолютной длине текста.\n",
    "\n",
    "\n",
    "### Зачем нужна нормализация TF-IDF\n",
    "\n",
    "1. **Проблема:**\n",
    "\n",
    "   * У нас есть два документа: один короткий, другой длинный.\n",
    "   * Даже если в них встречаются одинаковые слова с одинаковой частотой, длинный документ даст **большие числа TF-IDF**, просто потому что слов больше.\n",
    "   * Без нормализации длинный документ будет \"весить\" больше, хотя по сути информация та же.\n",
    "\n",
    "2. **Решение (нормализация L2):**\n",
    "\n",
    "   * Делим все числа TF-IDF в векторе на его **длину** (квадратный корень суммы квадратов всех элементов).\n",
    "   * Вектор становится длины 1, но **сохраняется пропорция между словами**.\n",
    "\n",
    "3. **Эффект:**\n",
    "\n",
    "   * Теперь сравнивать документы можно по **направлению векторного пространства** (какие слова важнее), а не по количеству слов.\n",
    "   * Длина документа больше не влияет на \"вес\" документа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a8daa-2e30-414c-8a42-d6022b965eee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Очистка текстовых данных\n",
    "\n",
    "**Задача** - удалить знаки препинания и другие небуквенные символы, так как они не содержат полезной семантики, хоть и иногда запятые предоставляют полезную информацию. Оставим только символы смайликов, поскольку полезны для анализа эмоциональной окраски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a288271-8cfc-4a39-b62d-770243c34626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olitics fairly funny, too!) Damon Runyon lives!!!!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# До очистки данных\n",
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd9426c3-e4b8-4b82-be0d-7a98cd9490cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Библиотека регулярных выражений для очистки\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Убираем HTML-теги\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    # Находим смайлы\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Убираем всё, кроме букв и цифр, добавляем смайлы\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d351e-8291-405e-9a12-4a7415963bf6",
   "metadata": {},
   "source": [
    "### Предобработка текста с помощью регулярных выражений\n",
    "\n",
    "1. **Удаление HTML-разметки**\n",
    "\n",
    "   * Используем простое регулярное выражение `<.*?>` для очистки текста от тегов.\n",
    "   * Для более сложного HTML можно использовать модуль `html.parser`.\n",
    "\n",
    "2. **Обработка смайликов**\n",
    "\n",
    "   * Сначала ищем смайлики с помощью регулярных выражений и временно сохраняем их.\n",
    "   * После очистки текста от всех несловесных символов (`[\\W]+`) добавляем смайлики в конец строки.\n",
    "   * Символ «носа» в смайликах (`:-)`) удаляется для единообразия.\n",
    "\n",
    "3. **Приведение текста к нижнему регистру**\n",
    "\n",
    "   * Заглавные буквы не считаются информативными для анализа тональности (теряем информацию об именах собственных, но упрощаем обработку).\n",
    "\n",
    "\n",
    "**Важно:** добавление смайликов в конец строки не нарушает модель мешка слов, так как порядок слов не учитывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02992f19-9fec-4ee8-bd73-f4c7bd003ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olitics fairly funny too damon runyon lives '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afe5869d-eb91-4256-b1fa-23884f16411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим нашу функцию ко всем документам в нашем DataFrame\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c944a666-e347-40a4-b613-030f7af91f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i just want to add my two cents worth and forg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this video nasty was initially banned in brita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the original grudge the original american rema...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>based on the manga comic of well known artist ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i d been interested in watching this ever sinc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  i just want to add my two cents worth and forg...          1\n",
       "1  this video nasty was initially banned in brita...          0\n",
       "2  the original grudge the original american rema...          0\n",
       "3  based on the manga comic of well known artist ...          1\n",
       "4  i d been interested in watching this ever sinc...          0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Убедимся в этом\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08bc83-8922-4307-b2e6-69b72708d962",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Получение токенов из документов \n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Токенизация и стемминг\n",
    "\n",
    "### Токенизация\n",
    "\n",
    "* **Цель:** разбить текст на отдельные элементы (токены).\n",
    "* **Простой способ:** разделить очищенный текст по пробелам (каждое слово = токен).\n",
    "\n",
    "### Стемминг (stemming)\n",
    "\n",
    "* **Что это:** преобразование слова в его корень (основу), чтобы объединять родственные слова.\n",
    "* **Пример:** `running → run`, `cats → cat`.\n",
    "* **Алгоритмы стемминга:**\n",
    "\n",
    "  * **Porter (классический, старый, простой)**\n",
    "  * **Snowball / Porter2 (быстрее, современнее)**\n",
    "  * **Lancaster (агрессивный, короткие и иногда неясные корни)**\n",
    "* Реализация в Python: пакет **NLTK** (`nltk.stem`)\n",
    "\n",
    "### Лемматизация vs Стемминг\n",
    "\n",
    "* **Лемматизация:** получает каноническую форму слова (грамматически правильную).\n",
    "* **Стемминг:** быстрее, но может давать «несуществующие» слова.\n",
    "* **На практике:** переход от стемминга к лемматизации почти не улучшает качество классификации текста.\n",
    "\n",
    "### Удаление стоп-слов\n",
    "\n",
    "* **Что это:** удаление слов, которые часто встречаются и мало помогают различать документы.\n",
    "* **Примеры стоп-слов:** `is, has, and, like`\n",
    "* **Когда полезно:** при работе с необработанными или нормализованными частотами терминов (не обязательно при TF-IDF, где частые слова уже получают меньший вес)\n",
    "* **В Python:** NLTK содержит готовый набор стоп-слов: `nltk.download`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46727705-95bb-403d-ab30-33ea175889b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токенизация\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "    \n",
    "#Пример\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdee4d04-7f40-44ed-b525-7031dca8698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Стемминг\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "# Пример \n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5a2e660-28eb-4d40-bd40-6af35cfd0e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удаление стоп слова\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "# Пример\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
    "  if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8b7e5-526d-4e15-b80e-a7550698a7fb",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Обучение модели логистической регрессии для классификации документов\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Классификация обзоров фильмов с помощью логистической регрессии\n",
    "\n",
    "### Данные\n",
    "\n",
    "* 25k документов → обучение\n",
    "* 25k документов → тест\n",
    "\n",
    "### Модель\n",
    "\n",
    "* Логистическая регрессия (две категории: положительные / отрицательные).\n",
    "* Решатель: **`liblinear`** (лучше подходит для больших наборов данных, чем `lbfgs`).\n",
    "\n",
    "### Поиск параметров\n",
    "\n",
    "* Используем **GridSearchCV** с 5-кратной стратифицированной кросс-валидацией.\n",
    "* Ограничиваем число комбинаций (иначе обучение слишком долго).\n",
    "\n",
    "### Векторизация текста\n",
    "\n",
    "* Вместо `CountVectorizer + TfidfTransformer` → используем **`TfidfVectorizer`** (объединяет оба шага).\n",
    "\n",
    "### Grid параметров\n",
    "\n",
    "* **Вариант 1:** `use_idf=True`, `smooth_idf=True`, `norm='l2'` (TF-IDF).\n",
    "* **Вариант 2:** `use_idf=False`, `smooth_idf=False`, `norm=None` (сырые частоты).\n",
    "\n",
    "### Регуляризация\n",
    "\n",
    "* Классификатор: регуляризация **L2**.\n",
    "* Подбираем параметр **C** (обратная сила регуляризации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4065b779-6463-4d20-953e-e5ca4753a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Время обучения: 11.62 минут (697.29 секунд)\n",
      "Лучшие параметры: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__norm': 'l2', 'vect__smooth_idf': True, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x750a509793a0>, 'vect__use_idf': True}\n",
      "Лучшая точность на кросс-валидации: 0.8949\n"
     ]
    }
   ],
   "source": [
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train = df.loc[:25000, 'review'].values \n",
    "X_test = df.loc[25000:, 'review'].values   \n",
    "y_train = df.loc[:25000, 'sentiment'].values  \n",
    "y_test = df.loc[25000:, 'sentiment'].values   \n",
    "\n",
    "# Определяем небольшой сет параметров для GridSearchCV\n",
    "small_param_grid = [\n",
    "    {\n",
    "        # Векторизация только униграмм (по одному слову)\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [None],  # не удаляем стоп-слова\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],  # используем два разных токенизатора\n",
    "        'clf__penalty': ['l2', 'l1'],  # разные виды регуляризации\n",
    "        'vect__use_idf': [True],       # использовать TF-IDF\n",
    "        'vect__smooth_idf': [True],    # сглаживание IDF\n",
    "        'vect__norm': ['l2'],          # L2-нормализация\n",
    "        'clf__C': [1.0, 10.0]          # разные коэффициенты регуляризации\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        'vect__stop_words': [stop, None],  # пробуем удалять стоп-слова или нет\n",
    "        'vect__tokenizer': [tokenizer],    # только один токенизатор\n",
    "        'vect__use_idf': [False],          # не использовать TF-IDF\n",
    "        'vect__smooth_idf': [False],       # без сглаживания\n",
    "        'vect__norm': [None],              # без нормализации\n",
    "        'clf__penalty': ['l2', 'l1'],      # регуляризация\n",
    "        'clf__C': [1.0, 10.0]              # коэффициенты регуляризации\n",
    "    },\n",
    "]\n",
    "\n",
    "# Создаем пайплайн: сначала TF-IDF преобразование, затем логистическая регрессия\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents=None,   # не удаляем акценты\n",
    "                             lowercase=False,     # не приводим к нижнему регистру\n",
    "                             preprocessor=None,   # не используем дополнительный препроцессор\n",
    "                             token_pattern=None)),# токенизация через кастомные токенизаторы\n",
    "    ('clf', LogisticRegression(solver='liblinear'))  # логистическая регрессия\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Настраиваем GridSearchCV для подбора лучших гиперпараметров\n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    lr_tfidf,               # модель\n",
    "    small_param_grid,       # сет параметров\n",
    "    scoring='accuracy',     # метрика для оценки качества\n",
    "    cv=5,                   # кросс-валидация с 5 фолдами\n",
    "    verbose=1,              # минимальный вывод прогресса\n",
    "    n_jobs=-1               # использовать все доступные ядра процессора\n",
    ")\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Время обучения: {(end - start)/60:.2f} минут ({end - start:.2f} секунд)\")\n",
    "\n",
    "print(f\"Лучшие параметры: {gs_lr_tfidf.best_params_}\")\n",
    "\n",
    "print(f\"Лучшая точность на кросс-валидации: {gs_lr_tfidf.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2f9d7cb-5935-4fee-8063-fec8f07a8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность на тествовом наборе: 0.901\n"
     ]
    }
   ],
   "source": [
    "# Тест оценка на лучшей модели\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Точность на тествовом наборе: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538e53e-a32f-4c61-a7b4-c74a30956dcd",
   "metadata": {},
   "source": [
    "Наилучшие результаты показала модель логистической регрессии с обычным `tokenizer` (без стемминга Портера и без стоп-слов), которая использовала **TF-IDF** для представления текста. Регуляризация была **L2** с параметром `C = 10.0`.\n",
    "\n",
    "* Точность на обучении: 89,5%\n",
    "* Точность на тесте: 90%\n",
    "\n",
    "То есть модель способна с высокой точностью предсказывать, является отзыв положительным или отрицательным.\n",
    "\n",
    "Наивный байесовский классификатор также широко используется для задач классификации текста, например, для фильтрации спама. Он прост в реализации, эффективен и хорошо работает на небольших наборах данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9fcf7a-ef12-4d37-9c88-95c56c482fd4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Работа с большими данными: онлайн-алгоритмы и внешнее обучение\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Проблема\n",
    "\n",
    "* Полное построение векторов признаков для 50 000+ документов требует много памяти.\n",
    "* Реальные наборы данных могут быть ещё больше и не помещаться в RAM.\n",
    "\n",
    "### Решение: внешнее обучение (out-of-core learning)\n",
    "\n",
    "* Обучение на данных, которые не умещаются в память, по частям (мини-пакетами).\n",
    "* Используется **`partial_fit`** у `SGDClassifier` в scikit-learn.\n",
    "\n",
    "### Основные шаги\n",
    "\n",
    "1. **Tokenizer** – очищает текст, разбивает на токены, удаляет стоп-слова.\n",
    "2. **Stream\\_docs** – генератор, возвращающий документы по одному.\n",
    "3. **Get\\_minibatch** – получает из потока определённое количество документов.\n",
    "\n",
    "### Ограничения обычных векторизаторов\n",
    "\n",
    "* `CountVectorizer` и `TfidfVectorizer` требуют хранения всего словаря и всех векторов признаков в памяти → не подходят для внешнего обучения.\n",
    "\n",
    "### Решение для внешнего обучения\n",
    "\n",
    "* Используется **`HashingVectorizer`**:\n",
    "\n",
    "  * Не хранит словарь в памяти\n",
    "  * Использует хеш-функцию (например, `murmurHash3`) для генерации индексов признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d03861e-4297-4f0e-bc3f-ffa0e8f64a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenized(text):\n",
    "    # Удаляем HTML-теги\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "\n",
    "    # Находим смайлики\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "    # Удаляем все несловесные символы и приводим к нижнему регистру\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "\n",
    "    # Разбиваем на токены и убираем стоп-слова\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ad176ac-1a58-4d65-b40c-c75ac10c5e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I just want to add my two cents worth, and forgive me if I am repeating something that has already been posted, but I feel it is worth reminding people of the everlovin\\' genius of Damon Runyon. Without the wonderfully street, hilarious writings of Damon Runyon this film would never have been made - nor most of the other great classics that deal with gamblers & the like from before 1960. Damon Runyon worked as a newspaper man, and he was from Colorado, but he sure did _get_ the street scene of the East Coast. If you are not a dedicated fan of old Hollywood comedies, I would recommend a few other flicks from Damon Runyon\\'s writings; \"\"the Lemondrop Kid,\"\" and \"\"Little Miss Marker,\"\" both feature Bob Hope, who, aside from his politics, has always been a funny man. (As a West Coast liberal, I find his politics fairly funny, too!) Damon Runyon lives!!!!\"',\n",
       " 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Генератор возвращающий документы по одному\n",
    "def stream_docs(path):\n",
    "    path = os.path.expanduser(path)\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv:\n",
    "        next(csv) # пропуск заголовка\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "# Проверим эту функцию прочитав первый документ из наших данных\n",
    "# который должен вернуть кортеж из текста и метки класса\n",
    "next(stream_docs(path = '~/ML/Data/movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d568a23a-78ee-4981-945e-a2684eee977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим функцию которая будет получать поток документов из\n",
    "# функции генератора возвращающих документов и возвращать\n",
    "# нужное кол-во документов через size\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2dc009a-4a39-4e04-9182-6ba58b0faf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внешнее обучения с HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21, # Кол-во признаков\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = tokenized)\n",
    "# Стохастический градиентный спуск с параметром потерь\n",
    "# логистической регрессии\n",
    "clf = SGDClassifier(loss = 'log_loss', random_state = 1)\n",
    "doc_stream = stream_docs(path = '~/ML/Data/movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17c9f0-6e81-4267-adef-b52f28fce7f7",
   "metadata": {},
   "source": [
    ">Выбирая большое кол-во признаков в `HashingVectorizer`, мы уменьньшаем вероятность возникновения коллизий хешей, но при этом увеличиваем кол-во коэффицентов в нашей модели логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e7c4736-29e6-4bb1-b5be-a70cf9333d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:16\n"
     ]
    }
   ],
   "source": [
    "# После настройки всех функций можем обучаться на внешних данных\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "839092fb-f413-4547-93c1-8134299d3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.877\n"
     ]
    }
   ],
   "source": [
    "# Оценка точности модели\n",
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Точность: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913450e1-445b-4c28-9cc3-6c890156e30d",
   "metadata": {},
   "source": [
    "### Онлайн-обучение модели (mini-batch)\n",
    "\n",
    "* Используем **PyPrind** для отображения прогресса обучения (45 делений).\n",
    "* Каждый мини-пакет = 1000 документов.\n",
    "* Последние 5000 документов используются для оценки точности.\n",
    "* **Ошибка NoneType:** возникает, если повторно вызвать генератор мини-пакетов — документы уже извлечены, поэтому `x_test` возвращает `None`. Решение: заново инициализировать генератор через `stream_docs(...)`.\n",
    "* **Точность модели:** \\~87% (немного ниже, чем при использовании GridSearchCV).\n",
    "* Преимущество: эффективное использование памяти, обучение занимает <1 минуты.\n",
    "\n",
    "### Алгоритм Word2Vec\n",
    "\n",
    "* **Тип:** обучение без учителя, нейронная сеть.\n",
    "* **Идея:** слова с похожим смыслом помещаются в близкие векторные кластеры.\n",
    "* **Особенность:** позволяет выполнять логические операции в векторном пространстве, например:\n",
    "\n",
    "  ```\n",
    "  король - мужчина + женщина ≈ королева\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5520ab-bc87-432e-80e4-9b043df1063a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Моделирование тем с использованием скрытого распределения Дирихле\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Моделирование тем (Topic Modeling)\n",
    "\n",
    "* **Цель:** присвоить немаркированным документам темы или категории (например, спорт, финансы, политика).\n",
    "* **Тип задачи:** обучение без учителя (классификация/кластеризация документов).\n",
    "\n",
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "* **Что это:** генеративная вероятностная модель для выявления групп слов, часто встречающихся вместе — эти группы отражают темы.\n",
    "* **Примечание:** не путать с Linear Discriminant Analysis (LDA) — метод контролируемого уменьшения размерности.\n",
    "* **Входные данные:** матрица мешка слов (Bag-of-Words).\n",
    "* **Выход:** две матрицы:\n",
    "\n",
    "  1. **Документ-тема** — насколько каждый документ относится к каждой теме.\n",
    "  2. **Слово-тема** — насколько каждое слово связано с каждой темой.\n",
    "* **Идея:** перемножением двух матриц восстанавливается исходная матрица мешка слов с минимальной ошибкой.\n",
    "* **Особенность:** нужно заранее задать количество тем (гиперпараметр).\n",
    "\n",
    "## Реализация LDA в scikit-learn\n",
    "\n",
    "* Класс: `LatentDirichletAllocation`\n",
    "* Пример применения: анализ и классификация набора обзоров фильмов.\n",
    "* В примере: ограничение на **10 тем**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38c1d977-aa31-4d59-9dc0-d99bee7e440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv('~/ML/Data/movie_data.csv', encoding = 'utf-8')\n",
    "\n",
    "# Применение CountVectorizer для создания матрицы набора слов\n",
    "# которая потсупит на вход LDA\n",
    "count = CountVectorizer(stop_words = 'english',\n",
    "                        max_df = .1,\n",
    "                        max_features = 5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "# Обучение оценщика на матрице набора слов\n",
    "lda = LatentDirichletAllocation(n_components = 10,\n",
    "                                random_state = 123,\n",
    "                                learning_method = 'batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "749e57c8-1604-496e-9a1e-d46747f703ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Матрица, содержащая важность слова\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1df577-bd3c-4a67-8d08-0cd5c4ec5ef1",
   "metadata": {},
   "source": [
    "### Настройка LDA и фильтрация слов\n",
    "\n",
    "### Ограничение частоты слов\n",
    "\n",
    "* `max_df=0.1` — исключаем слова, которые встречаются в более чем 10% документов.\n",
    "\n",
    "  * **Причина:** такие слова слишком общие и с меньшей вероятностью отражают конкретную тему.\n",
    "* `max_features=5000` — учитываем только 5000 наиболее часто встречающихся слов.\n",
    "\n",
    "  * **Причина:** ограничение размерности улучшает вычислительную эффективность и качество вывода модели.\n",
    "* **Примечание:** значения `max_df` и `max_features` выбраны произвольно и могут быть настроены при сравнении результатов.\n",
    "\n",
    "### Параметры обучения LDA\n",
    "\n",
    "* `learning_method='batch'` — модель обучается на всех данных за одну итерацию.\n",
    "\n",
    "  * Медленнее, чем `online` (мини-пакеты), но обычно точнее.\n",
    "* `learning_method='online'` — онлайн-обучение (мини-пакеты).\n",
    "\n",
    "### Алгоритм обучения\n",
    "\n",
    "* Используется **Expectation-Maximization (EM)** для итеративного обновления оценок параметров.\n",
    "\n",
    "### Результаты обучения\n",
    "\n",
    "* После обучения можно получить матрицу `components_` экземпляра `lda`:\n",
    "\n",
    "  * **Размерность:** количество слов × количество тем (например, 5000 × 10).\n",
    "  * **Содержание:** важность каждого слова для каждой темы (в порядке возрастания по темам).\n",
    "\n",
    "* Пример: обучение на матрице мешка слов с 10 темами может занять до 5 минут на ноутбуке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc849000-53a0-455c-b0c2-ad60f8270584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема 1:\n",
      "horror effects budget special gore\n",
      "Тема 2:\n",
      "guy money girl stupid girls\n",
      "Тема 3:\n",
      "version english action japanese match\n",
      "Тема 4:\n",
      "book read documentary game music\n",
      "Тема 5:\n",
      "series tv episode shows family\n",
      "Тема 6:\n",
      "family woman father mother wife\n",
      "Тема 7:\n",
      "music role performance musical star\n",
      "Тема 8:\n",
      "war police murder action men\n",
      "Тема 9:\n",
      "script actor worst minutes awful\n",
      "Тема 10:\n",
      "comedy action watched original fun\n"
     ]
    }
   ],
   "source": [
    "# Выведем пять самых важных слов для каждой из 10 тем\n",
    "# Значения важности слов ранжируются в порядке возрастания\n",
    "# Поэтому отсортируем массив тем в обратном порядке\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Тема {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                    [:-n_top_words -1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cce4b-010f-44a1-957b-de8e49fb7d86",
   "metadata": {},
   "source": [
    "Краткое описание каждой темы на основе ключевых слов `LDA`:\n",
    "\n",
    "| №  | Название темы          | Ключевые слова                            | Краткое описание                                                           |\n",
    "| -- | ---------------------- | ----------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| 1  | Ужасы                  | horror, effects, budget, special, gore    | Ужасы с акцентом на спецэффекты, кровь и бюджетные аспекты                 |\n",
    "| 2  | Молодежь и отношения   | guy, money, girl, stupid, girls           | Истории о парнях и девушках, социальные и повседневные ситуации            |\n",
    "| 3  | Боевики и экшн         | version, english, action, japanese, match | Боевики, версии фильмов на английском или японском, спортивные элементы    |\n",
    "| 4  | Документальные и книги | book, read, documentary, game, music      | Документальные фильмы, книги, музыка и игры                                |\n",
    "| 5  | Телесериалы            | series, tv, episode, shows, family        | Телесериалы и шоу, эпизоды, семейные истории                               |\n",
    "| 6  | Семья                  | family, woman, father, mother, wife       | Семейные темы, женские и отцовские роли, взаимоотношения в семье           |\n",
    "| 7  | Музыка и мюзиклы       | music, role, performance, musical, star   | Музыкальные фильмы и мюзиклы, роли, выступления, знаменитости              |\n",
    "| 8  | Криминал и боевики     | war, police, murder, action, men          | Боевики и криминальные сюжеты: война, полиция, убийства, мужские персонажи |\n",
    "| 9  | Критика фильмов        | script, actor, worst, minutes, awful      | Критика фильмов: сценарий, актеры, хронометраж, негативные аспекты         |\n",
    "| 10 | Комедии                | comedy, action, watched, original, fun    | Комедийные фильмы, боевики с юмором, оригинальные и развлекательные        |\n",
    "\n",
    "\n",
    "Чтобы убедиться, что категории осмысленно связаны с реальными обзорами, выведем текст отзывов на три фильма из категории **фильмы ужасов**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80f552a1-2f52-4f01-9115-cf4a7bdeb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фильм ужасов #1:\n",
      "Title: Zombie 3 (1988) <br /><br />Directors: Mostly Lucio Fulci, but also Claudio Fragasso and Bruno Mattei <br /><br />Cast: Ottaviano DellAcqua, Massimo Vani, Beatrice Ring, Deran Serafin <br /><br />Review: <br /><br />To review this flick and get some good background of it, I gotta start by the ...\n",
      "\n",
      "Фильм ужасов #2:\n",
      "Over Christmas break, a group of college friends stay behind to help prepare the dorms to be torn down and replaced by apartment buildings. To make the work a bit more difficult, a murderous, Chucks-wearing psycho is wandering the halls of the dorm, preying on the group in various violent ways.<br / ...\n",
      "\n",
      "Фильм ужасов #3:\n",
      "Granted, HOTD 2 is better than the Uwe Boll crapfest that was the first one, but thats like saying drowning is better than being chopped alive. OK OK, I'm being a little bit harsh with this one, its just that Video Game adaptations of Zombie movies always leave a bad taste in my mouth. Resident Evil ...\n"
     ]
    }
   ],
   "source": [
    "proverka = X_topics[:, 0].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(proverka[:3]):\n",
    "    print(f'\\nФильм ужасов #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af550a-ca09-4d03-9f3e-c90e3526a8dd",
   "metadata": {},
   "source": [
    "**Фильм ужасов №1:**\n",
    "**Название:** Zombie 3 (1988)\n",
    "**Режиссёры:** В основном Лучо Фульчи, а также Клаудио Фрагассо и Бруно Маттеи\n",
    "**В ролях:** Оттавиано Делл’Аккуа, Массимо Вани, Беатрис Ринг, Деран Серафин\n",
    "**Обзор:**\n",
    "Чтобы сделать обзор этого фильма и дать некоторый контекст, мне нужно начать с того, что...\n",
    "\n",
    "**Фильм ужасов №2:**\n",
    "На рождественских каникулах группа студентов остаётся, чтобы помочь подготовить общежития к сносу и замене их на жилые дома. Чтобы сделать работу немного сложнее, по коридорам общежития бродит убийца-психопат в обуви Чака, который охотится на группу различными жестокими способами.\n",
    "\n",
    "\n",
    "**Фильм ужасов №3:**\n",
    "Конечно, HOTD 2 лучше, чем ужасный фильм Уве Болла, которым был первый, но это всё равно, что сказать «тонуть лучше, чем быть живьём разрубленным». Ладно-ладно, я слишком резок, просто адаптации видеоигр про зомби-фильмы всегда оставляют неприятный осадок во рту. Resident Evil...\n",
    "\n",
    "---\n",
    "\n",
    ">Мы посмотрели первые 300 символов для трех наиболее популярных фильмов ужасов. Рецензии - отзывы звучат действительно как для фильмов ужасов.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Итоги:\n",
    "\n",
    "1. **Анализ настроений (Sentiment Analysis)**\n",
    "\n",
    "   * Классификация текстов по эмоциональной окраске.\n",
    "   * Используются алгоритмы машинного обучения, такие как логистическая регрессия и наивный байес.\n",
    "\n",
    "2. **Представление текста в виде признаков**\n",
    "\n",
    "   * **Мешок слов (Bag-of-Words):** каждый документ кодируется как вектор частот слов.\n",
    "   * **TF-IDF:** взвешивание частоты термина по его релевантности, чтобы важные слова имели больший вес.\n",
    "\n",
    "3. **Особенности работы с текстовыми данными**\n",
    "\n",
    "   * Большие векторы признаков требуют значительных вычислительных ресурсов.\n",
    "\n",
    "4. **Моделирование тем (Topic Modeling)**\n",
    "\n",
    "   * Метод: **Latent Dirichlet Allocation (LDA)**.\n",
    "   * Применение: распределение обзоров фильмов по темам без меток (обучение без учителя)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Lab",
   "language": "python",
   "name": "my-ml-toolkit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
