{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a8fe88-80f8-4120-a46e-ee9e4006c4c2",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Градиентный бустинг: обучение ансамбля на основе градиентов потерь\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## **Градиентный бустинг (Gradient Boosting)**\n",
    "\n",
    "### Определение\n",
    "\n",
    "* Вариант бустинга: **последовательное обучение слабых учеников** для создания сильного ансамбля.\n",
    "\n",
    "\n",
    "### Важность\n",
    "\n",
    "* Один из **ключевых методов** в современном ML.\n",
    "* Широко применяется в соревнованиях (например, **Kaggle**) и реальных проектах.\n",
    "\n",
    "---\n",
    "\n",
    "## **Сравнение AdaBoost и Gradient Boosting**\n",
    "\n",
    "| Характеристика           | **AdaBoost**                                                                                                  | **Gradient Boosting**                                                                                       |\n",
    "| ------------------------ | ------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| **Базовые модели**       | Обычно **обрубки деревьев** (decision stumps, глубина = 1)                                                    | Деревья глубже (глубина ≈ 3–6 или 8–64 листьев)                                                             |\n",
    "| **Подход к обучению**    | Итеративно обучает классификаторы на **взвешенных данных** — ошибки предыдущей модели влияют на веса примеров | Итеративно обучает деревья на **остатках ошибок** (градиентах) — ошибки становятся новой целевой переменной |\n",
    "| **Веса**                 | Индивидуальные веса для каждого примера и каждого классификатора                                              | **Глобальная скорость обучения** (learning rate), одинаковая для всех деревьев                              |\n",
    "| **Критерий остановки**   | Достижение максимального числа итераций                                                                       | Достижение максимального числа деревьев или другой критерий                                                 |\n",
    "| **Интерпретация ошибок** | Используются напрямую для перераспределения весов                                                             | Преобразуются в новые \"целевые\" значения для следующей модели                                               |\n",
    "\n",
    "### Главное отличие в механизме\n",
    "\n",
    "* **AdaBoost**: меняет **веса примеров** → слабые ученики фокусируются на сложных объектах.\n",
    "* **Gradient Boosting**: строит новые модели, минимизируя **градиент функции потерь** → ошибки становятся новым таргетом.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Градиентный бустинг — общий алгоритм (пример: бинарная классификация)**\n",
    "\n",
    "### Идея\n",
    "\n",
    "* Строим последовательность деревьев решений.\n",
    "* Каждое дерево обучается на **ошибках (остатках)** предыдущего ансамбля.\n",
    "* Ошибки определяются как **отрицательный градиент функции потерь**.\n",
    "* Каждое новое дерево делает **небольшой шаг** в направлении уменьшения ошибки (через *learning rate*).\n",
    "\n",
    "\n",
    "### **Пошаговый алгоритм**\n",
    "\n",
    "#### **Шаг 1 — Инициализация**\n",
    "\n",
    "* Строим **одноузловое дерево** (корневой узел), выдающее константу $y_0$.\n",
    "* Константа находится как:\n",
    "\n",
    "$$\n",
    "F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma)\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $L$ — функция потерь,\n",
    "* $n$ — число примеров.\n",
    "\n",
    "\n",
    "#### **Шаг 2 — Итерации (для $m = 1, \\dots, M$)**\n",
    "\n",
    "**a. Вычисляем псевдоостатки (pseudo-residuals)**:\n",
    "\n",
    "$$\n",
    "r_{im} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x)=F_{m-1}(x)}\n",
    "$$\n",
    "\n",
    "**b. Строим дерево решений** по $r_{im}$ как целевой переменной.\n",
    "Пусть дерево имеет $J_m$ листьев с областями $R_{jm}$.\n",
    "\n",
    "**c. Для каждого листа $R_{jm}$**:\n",
    "\n",
    "$$\n",
    "\\gamma_{jm} = \\arg\\min_{\\gamma} \\sum_{x_i \\in R_{jm}} L\\big(y_i, F_{m-1}(x_i) + \\gamma\\big)\n",
    "$$\n",
    "\n",
    "— вычисляем оптимальное смещение $\\gamma_{jm}$, минимизирующее функцию потерь.\n",
    "\n",
    "**d. Обновляем модель**:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\cdot \\sum_{j=1}^{J_m} \\gamma_{jm} \\cdot \\mathbf{1}(x \\in R_{jm})\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "* $\\eta$ — **скорость обучения** (*learning rate*, 0.01–1),\n",
    "* $\\mathbf{1}(\\cdot)$ — индикатор принадлежности объекта листу.\n",
    "\n",
    "### Особенности\n",
    "\n",
    "* **Псевдоостатки** = направление, в котором нужно улучшить прогноз (градиент потерь).\n",
    "* **Learning rate ($\\eta$)**: замедляет обновление, снижает риск переобучения.\n",
    "* **Глубина деревьев**: обычно 3–6, но можно настраивать.\n",
    "\n",
    "---\n",
    "\n",
    "## **Практические реализации градиентного бустинга**\n",
    "\n",
    "### Проблема классической версии\n",
    "\n",
    "* Градиентный бустинг — **последовательный процесс**, обучение может быть **медленным**.\n",
    "\n",
    "\n",
    "### **XGBoost (eXtreme Gradient Boosting)**\n",
    "\n",
    "* **Оптимизации**:\n",
    "\n",
    "  * Приёмы и приближения для ускорения обучения.\n",
    "  * Эффективное использование памяти.\n",
    "  * Регуляризация для борьбы с переобучением.\n",
    "* **Сильные стороны**:\n",
    "\n",
    "  * Высокая скорость обучения.\n",
    "  * Отличные прогностические характеристики.\n",
    "* **Популярность**:\n",
    "\n",
    "  * Победы во многих соревнованиях Kaggle.\n",
    "\n",
    "### Другие реализации\n",
    "\n",
    "* **LightGBM** — от Microsoft, ориентирован на скорость и низкое потребление памяти.\n",
    "* **CatBoost** — от Яндекса, лучше работает с категориальными признаками \"из коробки\".\n",
    "* **HistGradientBoostingClassifier** (scikit-learn) — основан на подходах LightGBM, быстрее классического `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b4e45e-2587-4dad-9543-829947dbab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost соответсвует API scikit-learn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f64250-3cf7-4444-aa1a-3af38db47e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "                      'machine-learning-databases/'\n",
    "                      'wine/wine.data',\n",
    "                      header = None)\n",
    "df_wine.columns = ['Class label', 'Alcohol',\n",
    "                   'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium',\n",
    "                   'Total phenols', 'Flavanoids',\n",
    "                   'Nonflavanoid phenols',\n",
    "                   'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "# Отбрасываем класс 1\n",
    "df_wine = df_wine[df_wine['Class label'] != 1]\n",
    "\n",
    "# Разделение данных\n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].values\n",
    "y = df_wine['Class label'].values\n",
    "\n",
    "df_wine[['Class label', 'Alcohol', 'OD280/OD315 of diluted wines']].head()\n",
    "\n",
    "# Кодирование меток класса в двоичный формат\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, test_size = 0.2,\n",
    "                     random_state = 1,\n",
    "                     stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aed5ebb-7c76-4a05-a72e-f2012655ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность XGBoost при обучении/тестировании 0.968 / 0.917\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(n_estimators = 1000, \n",
    "                          learning_rate = 0.01,\n",
    "                          max_depth = 4,\n",
    "                          random_state = 1)\n",
    "\n",
    "gbm = model.fit(X_train, y_train)\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "y_test_pred = gbm.predict(X_test)\n",
    "gbm_train = accuracy_score(y_train, y_train_pred)\n",
    "gbm_test = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Точность XGBoost при обучении/тестировании '\n",
    "      f'{gbm_train:.3f} / {gbm_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d6130-c6cf-46bf-9065-341f703c8f28",
   "metadata": {},
   "source": [
    "Мы обучили классификатор на основе градиентного бустинга с `1000 деревьев (раундов)` и `скоростью обучения = 0.001`. Посколько мы поощряем слабых учеников, значения от 2 до 6 является разумнынм выбором.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Вывод по ансамблевым методам**\n",
    "\n",
    "### **Общая идея ансамблей**\n",
    "\n",
    "* Ансамблевые методы объединяют **несколько моделей**, чтобы компенсировать их слабые стороны.\n",
    "* Результат — **стабильные и точные модели**, полезные как для **производственных задач**, так и для **соревнований по машинному обучению**.\n",
    "\n",
    "\n",
    "### **Мажоритарное голосование (Majority Voting)**\n",
    "\n",
    "* Простейший способ объединения предсказаний нескольких моделей.\n",
    "* Каждая модель делает прогноз, а **итоговое решение выбирается по большинству голосов**.\n",
    "* Используется как базовый метод для ансамблей и для **бэггинга**.\n",
    "\n",
    "\n",
    "### **Бэггинг (Bagging)**\n",
    "\n",
    "* Уменьшает **дисперсию модели** за счет:\n",
    "\n",
    "  * Создания **случайных бутстрэп-выборок** из обучающего набора.\n",
    "  * Обучения **нескольких независимых моделей**.\n",
    "  * Объединения их **по принципу мажоритарного голосования**.\n",
    "* Основной эффект: **устойчивость модели к шуму** и переобучению.\n",
    "\n",
    "\n",
    "### **Бустинг**\n",
    "\n",
    "* Использует **последовательное обучение слабых учеников**, которые учатся на ошибках предыдущих моделей.\n",
    "* Основные виды:\n",
    "\n",
    "  * **AdaBoost**:\n",
    "\n",
    "    * Взвешивает ошибки выборки для каждой итерации.\n",
    "    * Слабые модели (обычно обрубки деревьев решений) комбинируются в сильный классификатор.\n",
    "  * **Градиентный бустинг**:\n",
    "\n",
    "    * Постепенно корректирует ошибки предыдущих деревьев через **градиент функции потерь**.\n",
    "    * Популярные реализации:\n",
    "\n",
    "      * **XGBoost** — ускоренная и регуляризованная версия градиентного бустинга.\n",
    "      * **LightGBM** — оптимизация скорости и памяти.\n",
    "      * **CatBoost** — удобен для категориальных признаков.\n",
    "      * **HistGradientBoostingClassifier** (scikit-learn) — эффективная версия градиентного бустинга.\n",
    "\n",
    "\n",
    "**Итог**:\n",
    "\n",
    "* Мажоритарное голосование → простое объединение моделей.\n",
    "* Бэггинг → уменьшение дисперсии через независимые выборки и голосование.\n",
    "* Бустинг → последовательное обучение на ошибках для повышения точности.\n",
    "* Ансамбли повышают **качество прогнозов**, но могут увеличивать **вычислительные затраты**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Lab",
   "language": "python",
   "name": "my-ml-toolkit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
