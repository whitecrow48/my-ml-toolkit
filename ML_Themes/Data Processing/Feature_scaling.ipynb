{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbacfa79-9d50-4a24-b10a-5cf09d20a56f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Приведение признаков к одному масштабу\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Масштабирование признаков (feature scaling) — обязательный шаг в предобработке, который улучшает работу большинства алгоритмов (градиентный спуск, kNN, SVM), за исключением некоторых деревьев решений и случайных лесов (они инвариантны к масштабу). Без приведения признаков к единому масштабу весовые обновления или вычисления расстояний будут доминироваться признаками с большими величинами.\n",
    "\n",
    "Существует два основных подхода:\n",
    "\n",
    "1. **Нормализация (min–max scaling)**\n",
    "   Приводит значения каждого признака к диапазону $[0,1]$. Мы применяем минимаксное\n",
    "масштабирование к каждому столбцу признаков, где новое значение $x^{(i)}_{\\text{norm}}$\n",
    " записи $x^{(i)}$ можно рассчитать как:\n",
    "\n",
    "   $$\n",
    "     x^{(i)}_{\\text{norm}}\n",
    "     = \\frac{x^{(i)} - x_{\\min}}{x_{\\max} - x_{\\min}}.\n",
    "   $$\n",
    "\n",
    "2. **Стандартизация (z‑scoring)**\n",
    "   Стандартизация особенно полезна для линейных моделей, таких как логистическая регрессия и SVM, потому что они инициализируют веса с нуля или малыми значениями.\n",
    "\n",
    "   Когда признаки имеют нулевое среднее и единичную дисперсию, как при стандартизации, это упрощает обучение: градиенты становятся более сбалансированными, и модель быстрее и стабильнее сходится.\n",
    "\n",
    "   Важно понимать:\n",
    "\n",
    "   * Стандартизация не меняет форму распределения признаков (не делает данные \"нормальными\").\n",
    "   * В отличие от min–max нормализации, она не ограничивает значения, а сохраняет информацию о выбросах, делая модель менее чувствительной к крайним значениям.\n",
    "   Процедура стандартизации может быть выражена следующим уравнением:\n",
    "\n",
    "   $$\n",
    "     x^{(i)}_{\\text{std}}\n",
    "     = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}.\n",
    "   $$\n",
    "\n",
    "   Здесь $\\mu_x$ - выборочное среднее для столбца признака, а $\\sigma_x$ - стандартное отклонение.\n",
    "\n",
    "   Значения после стандартизации могут быть отрицательными или положительными, потому что данные приводятся к среднему 0 и стандартному отклонению 1. Всё, что меньше среднего — даёт отрицательные значения, больше — положительные.\n",
    "\n",
    "\n",
    "**Когда что выбирать?**\n",
    "\n",
    "* Min–max подходит, если нужно жёсткое ограничение диапазона (например, при работе с изображениями).\n",
    "* Стандартизация чаще предпочтительна для линейных моделей и алгоритмов оптимизации: она сохраняет форму распределения, менее чувствительна к выбросам и упрощает сходимость градиентного спуска.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe552494-50cc-4583-a1b8-bfd6293201f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_norm (первые 2 строки):\n",
      "[[0.64619883 0.83201581 0.4248366  0.46236559 0.27160494 0.35172414\n",
      "  0.09704641 0.68       0.18987342 0.23623446 0.45744681 0.28571429\n",
      "  0.19400856]\n",
      " [0.6871345  0.15612648 0.65359477 0.43548387 0.7654321  0.67931034\n",
      "  0.50632911 0.74       0.2943038  0.3250444  0.81914894 0.63369963\n",
      "  0.68259629]]\n",
      "\n",
      "X_test_norm (первые 2 строки):\n",
      "[[0.69005848 0.22924901 0.64052288 0.30645161 0.55555556 0.69655172\n",
      "  0.51687764 0.52       0.39873418 0.40497336 0.69148936 0.60805861\n",
      "  0.78245364]\n",
      " [0.22222222 0.14031621 0.54248366 0.40860215 0.41975309 0.3137931\n",
      "  0.29746835 0.64       0.19303797 0.10746004 1.03191489 0.35164835\n",
      "  0.05492154]]\n"
     ]
    }
   ],
   "source": [
    "# Процедура нормализации (минимаксного масштабирования) в scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузка датасета Wine из репозитория UCI в DataFrame без заголовков\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                      'ml/machine-learning-databases/'\n",
    "                      'wine/wine.data', header=None)\n",
    "\n",
    "# Назначение имен столбцам для лучшей читаемости и удобства\n",
    "df_wine.columns = ['Class label',        # Класс вина (1, 2 или 3)\n",
    "                   'Alcohol',            # Содержание алкоголя\n",
    "                   'Malic acid',         # Яблочная кислота\n",
    "                   'Ash',                # Зольность\n",
    "                   'Alcalinity of ash',  # Щелочность золы\n",
    "                   'Magnesium',          # Магний\n",
    "                   'Total phenols',      # Общее количество фенолов\n",
    "                   'Flavanoids',         # Флавоноиды\n",
    "                   'Nonflavanoid phenols',       # Нефлавоноидные фенолы\n",
    "                   'Proanthocyanins',             # Проантоцианы\n",
    "                   'Color intensity',             # Интенсивность цвета\n",
    "                   'Hue',                         # Оттенок\n",
    "                   'OD280/OD315 of diluted wines',# Показатель OD280/OD315\n",
    "                   'Proline']   \n",
    "\n",
    "#Разделение данных на обучающие и тестовые выборки\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y,\n",
    "                     test_size = 0.3,\n",
    "                     random_state = 0,\n",
    "                     stratify = y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train_norm (первые 2 строки):\")\n",
    "print(X_train_norm[:2])\n",
    "\n",
    "print(\"\\nX_test_norm (первые 2 строки):\")\n",
    "print(X_test_norm[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a03506c1-fd54-4b53-b53e-b99eb52be7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std (первые 2 строки):\n",
      "[[ 0.71225893  2.22048673 -0.13025864  0.05962872 -0.50432733 -0.52831584\n",
      "  -1.24000033  0.84118003 -1.05215112 -0.29218864 -0.20017028 -0.82164144\n",
      "  -0.62946362]\n",
      " [ 0.88229214 -0.70457155  1.17533605 -0.09065504  2.34147876  1.01675879\n",
      "   0.66299475  1.0887425  -0.49293533  0.13152077  1.33982592  0.54931269\n",
      "   1.47568796]]\n",
      "\n",
      "X_test_std (первые 2 строки):\n",
      "[[ 1.09517886 -0.40674741  1.26476181 -0.52362169  0.94557352  1.20882673\n",
      "   0.89897451  0.32761624  0.40358313  0.59874588  0.67823753  0.45328836\n",
      "   2.26479759]\n",
      " [-0.97853259 -0.8686148   0.73146818  0.0648075   0.18962376 -0.52667205\n",
      "  -0.21306092  0.79114314 -0.88304429 -0.92743603  1.99301704 -0.49127063\n",
      "  -1.30945676]]\n"
     ]
    }
   ],
   "source": [
    "# Процедура стандартизации\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.fit_transform(X_test)\n",
    "\n",
    "print(\"X_train_std (первые 2 строки):\")\n",
    "print(X_train_std[:2])\n",
    "\n",
    "print(\"\\nX_test_std (первые 2 строки):\")\n",
    "print(X_test_std[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c463f-b8b6-47c5-9658-92c6410bdb3c",
   "metadata": {},
   "source": [
    "Важно помнить: **масштабирование нужно настраивать только на обучающих данных**. То есть `StandardScaler` (или другой масштабировщик) мы обучаем один раз на тренировочном наборе, а затем используем те же параметры для преобразования тестовых данных или новых примеров. Это предотвращает утечку информации.\n",
    "Это важно, потому что тестовые данные должны оставаться \"невидимыми\" для модели до оценки. Если вы вычислите среднее и отклонение по всему датасету (включая тест), это приведёт к утечке информации из теста в обучение. В результате модель получит преимущество, которого не будет на реальных данных, и ваша оценка точности будет обманчиво завышена.\n",
    "\n",
    "Кроме стандартной стандартизации, в `scikit-learn` есть и другие масштабировщики. Один из них — **`RobustScaler`**, который особенно полезен при:\n",
    "\n",
    "* наличии **выбросов**,\n",
    "* работе с **небольшими датасетами**,\n",
    "* склонности модели к **переобучению**.\n",
    "\n",
    "`RobustScaler` работает по столбцам:\n",
    "он **вычитает медиану** и масштабирует данные по межквартильному размаху (между 1-м и 3-м квартилем), что делает масштабирование **устойчивым к выбросам**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Lab",
   "language": "python",
   "name": "my-ml-toolkit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
